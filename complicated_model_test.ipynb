{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline \r\n",
    "import torch\r\n",
    "from torch import nn, optim\r\n",
    "from torch.utils import data\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# データの読み込み\r\n",
    "train_data = pd.read_csv(\"NKI_RS_train_data.csv\",index_col=0)\r\n",
    "train_label = pd.read_csv(\"NKI_RS_train_label.csv\",index_col = 0)\r\n",
    "test_data = pd.read_csv(\"NKI_RS_test_data.csv\", index_col = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_label = train_label[\"sex\"]\r\n",
    "train_label"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "subject_id\n",
       "A00054153    0\n",
       "A00040383    1\n",
       "A00035337    1\n",
       "A00064053    1\n",
       "A00035840    1\n",
       "            ..\n",
       "A00037377    1\n",
       "A00057862    1\n",
       "A00059527    1\n",
       "A00061790    1\n",
       "A00039159    1\n",
       "Name: sex, Length: 810, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "learning_rate = 1e-2\r\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# modelのロード\r\n",
    "from model_test import Net\r\n",
    "model = Net()\r\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Net(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=6670, out_features=3000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3000, out_features=1500, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1500, out_features=750, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=750, out_features=100, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=100, out_features=20, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropoutを組み込んだモデルでテスト"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from CV_train_eval import cross_val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 0)\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "num = 0\r\n",
    "n_epochs = 100\r\n",
    "\r\n",
    "for train_idx, valid_idx in skf.split(train_data, train_label):\r\n",
    "    print(f\"This is {num + 1}th fold.--------------------\")\r\n",
    "    num += 1\r\n",
    "    # foldごとのモデルの初期化\r\n",
    "    model = Net().to(device)\r\n",
    "    # foldごとにモデルの最適化手法のパラメータ引数をちゃんと設定してあげる必要があった\r\n",
    "    optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9, weight_decay = 5e-3)\r\n",
    "\r\n",
    "    # データの準備-------------------------------------------------------------\r\n",
    "    # train_idx, valid_idxにはインデックスの値が格納されている。\r\n",
    "    _train_data = train_data.iloc[train_idx]\r\n",
    "    _train_label = train_label.iloc[train_idx]\r\n",
    "    _valid_data = train_data.iloc[valid_idx]\r\n",
    "    _valid_label = train_label.iloc[valid_idx]\r\n",
    "    # データの準備\r\n",
    "    # 訓練データをtensor型に変換------------\r\n",
    "    train_data_tensor = torch.tensor(np.array(_train_data.astype('f'))) # データのみ\r\n",
    "    train_label_tensor = torch.tensor(_train_label)\r\n",
    "    # DataLoaderに渡す\r\n",
    "    train_tensor = data.TensorDataset(train_data_tensor, train_label_tensor)\r\n",
    "    trainloader = data.DataLoader(train_tensor, batch_size = 64)\r\n",
    "    # valid\r\n",
    "    valid_data_tensor = torch.tensor(np.array(_valid_data.astype('f'))) # データのみ\r\n",
    "    valid_label_tensor = torch.tensor(_valid_label)\r\n",
    "    # print(valid_data_tensor.size(),valid_label_tensor.size())\r\n",
    "    # print(\"---------\")\r\n",
    "    valid_tensor = data.TensorDataset(valid_data_tensor, valid_label_tensor)\r\n",
    "    validloader = data.DataLoader(valid_tensor, batch_size = 64)\r\n",
    "    # ここまででデータの準備完了------------------------------------------------\r\n",
    "    # ここから学習\r\n",
    "    \r\n",
    "    for epoch in range(n_epochs):\r\n",
    "        train_loss = 0\r\n",
    "        train_acc = 0\r\n",
    "        valid_loss = 0\r\n",
    "        valid_acc = 0\r\n",
    "        auc_score = 0\r\n",
    "        \r\n",
    "        model.train() # 学習モード <-これテキストとかにあった気がしないがなんだ？？\r\n",
    "        # dropoutやbatch normのふるまいが変わる。今回はNNに組み込んでいないから必要ないと思われる\r\n",
    "        batch_count = 0\r\n",
    "        for xt, yt in trainloader: # ミニバッチずつ計算\r\n",
    "            # データをgpuに <-これであってる？？\r\n",
    "            xt = xt.to(device)\r\n",
    "            yt = yt.to(device)\r\n",
    "            \r\n",
    "            y_pred = model.forward(xt) # モデルにミニバッチをぶち込む\r\n",
    "            loss = loss_fn(y_pred, yt) # 予測結果の損失計算\r\n",
    "            # print(f\"{batch_count + 1}回目のミニバッチごとのロス:{loss}\")\r\n",
    "            train_loss += loss.item() * xt.size(0) # ミニバッチのサイズを掛ける\r\n",
    "            train_acc += (y_pred.max(1)[1] == yt).sum().item()\r\n",
    "\r\n",
    "            optimizer.zero_grad() # 勾配情報の初期化\r\n",
    "            loss.backward() # 誤差逆伝搬\r\n",
    "            optimizer.step() # 重みの更新\r\n",
    "            # print(batch_count)\r\n",
    "            batch_count += 1\r\n",
    "        # print(batch_count)\r\n",
    "        \r\n",
    "        avg_train_loss = train_loss / len(trainloader.dataset)\r\n",
    "        avg_train_acc = train_acc / len(trainloader.dataset)\r\n",
    "        \r\n",
    "        # validation\r\n",
    "        model.eval() # <- dropoutやbatch normのふるまいが変わる。今回はNNに組み込んでいないから必要ないと思われる\r\n",
    "        with torch.no_grad(): # validationにおいては勾配を計算する必要がない\r\n",
    "            for xv, yv in validloader:\r\n",
    "                xv = xv.to(device)\r\n",
    "                yv = yv.to(device)\r\n",
    "                y_pred = model(xv)\r\n",
    "                loss = loss_fn(y_pred, yv)\r\n",
    "                valid_loss += loss.item() * xv.size(0) # ミニバッチのサイズを掛ける\r\n",
    "                valid_acc += (y_pred.max(1)[1] == yv).sum().item()\r\n",
    "                \r\n",
    "\r\n",
    "            avg_valid_loss = valid_loss / len(validloader.dataset)\r\n",
    "            avg_valid_acc = valid_acc / len(validloader.dataset)\r\n",
    "            avg_auc_score = auc_score / len(validloader.dataset)\r\n",
    "        if epoch == 0 or (epoch + 1) % 50 == 0:\r\n",
    "            print(f\"epoch:{epoch + 1},train_loss:{avg_train_loss:.5f}, train_acc:{avg_train_acc:.5f}, val_loss:{avg_valid_loss:.5f},val_acc:{avg_valid_acc:.5f}\")\r\n",
    "        \r\n",
    "# 1epochを経ても重みが更新できていない\r\n",
    "# そもそもlossの値が小さすぎるような気がする"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is 1th fold.--------------------\n",
      "epoch:1,train_loss:0.68682, train_acc:0.60802, val_loss:0.68525,val_acc:0.60494\n",
      "epoch:50,train_loss:0.66940, train_acc:0.60802, val_loss:0.67084,val_acc:0.60494\n",
      "epoch:100,train_loss:0.66943, train_acc:0.60802, val_loss:0.67084,val_acc:0.60494\n",
      "This is 2th fold.--------------------\n",
      "epoch:1,train_loss:0.69959, train_acc:0.39198, val_loss:0.69641,val_acc:0.39506\n",
      "epoch:50,train_loss:0.66941, train_acc:0.60802, val_loss:0.67090,val_acc:0.60494\n",
      "epoch:100,train_loss:0.66943, train_acc:0.60802, val_loss:0.67090,val_acc:0.60494\n",
      "This is 3th fold.--------------------\n",
      "epoch:1,train_loss:0.69266, train_acc:0.52778, val_loss:0.69044,val_acc:0.60494\n",
      "epoch:50,train_loss:0.66960, train_acc:0.60802, val_loss:0.67093,val_acc:0.60494\n",
      "epoch:100,train_loss:0.66964, train_acc:0.60802, val_loss:0.67095,val_acc:0.60494\n",
      "This is 4th fold.--------------------\n",
      "epoch:1,train_loss:0.69132, train_acc:0.60648, val_loss:0.68938,val_acc:0.61111\n",
      "epoch:50,train_loss:0.67035, train_acc:0.60648, val_loss:0.66824,val_acc:0.61111\n",
      "epoch:100,train_loss:0.67036, train_acc:0.60648, val_loss:0.66825,val_acc:0.61111\n",
      "This is 5th fold.--------------------\n",
      "epoch:1,train_loss:0.68878, train_acc:0.60648, val_loss:0.68697,val_acc:0.61111\n",
      "epoch:50,train_loss:0.67030, train_acc:0.60648, val_loss:0.66837,val_acc:0.61111\n",
      "epoch:100,train_loss:0.67032, train_acc:0.60648, val_loss:0.66838,val_acc:0.61111\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from model import Net_simple, Net_dp, Net_bn\r\n",
    "model_dp_test = Net_dp()\r\n",
    "model_dp_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Net_dp(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=6670, out_features=3000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=3000, out_features=1500, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=1500, out_features=750, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=750, out_features=100, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Linear(in_features=100, out_features=20, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.2, inplace=False)\n",
       "    (15): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# dropoutを入れたやつ\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 0)\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "num = 0\r\n",
    "n_epochs = 100\r\n",
    "for train_idx, valid_idx in skf.split(train_data, train_label):\r\n",
    "    print(f\"This is {num + 1}th fold.--------------------\")\r\n",
    "    num += 1\r\n",
    "    # foldごとのモデルの初期化\r\n",
    "    model = Net_dp().to(device)\r\n",
    "    # foldごとにモデルの最適化手法のパラメータ引数をちゃんと設定してあげる必要があった\r\n",
    "    optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9, weight_decay = 5e-3)\r\n",
    "\r\n",
    "    # データの準備-------------------------------------------------------------\r\n",
    "    # train_idx, valid_idxにはインデックスの値が格納されている。\r\n",
    "    _train_data = train_data.iloc[train_idx]\r\n",
    "    _train_label = train_label.iloc[train_idx]\r\n",
    "    _valid_data = train_data.iloc[valid_idx]\r\n",
    "    _valid_label = train_label.iloc[valid_idx]\r\n",
    "    # データの準備\r\n",
    "    # 訓練データをtensor型に変換------------\r\n",
    "    train_data_tensor = torch.tensor(np.array(_train_data.astype('f'))) # データのみ\r\n",
    "    train_label_tensor = torch.tensor(_train_label)\r\n",
    "    # DataLoaderに渡す\r\n",
    "    train_tensor = data.TensorDataset(train_data_tensor, train_label_tensor)\r\n",
    "    trainloader = data.DataLoader(train_tensor, batch_size = 64)\r\n",
    "    # valid\r\n",
    "    valid_data_tensor = torch.tensor(np.array(_valid_data.astype('f'))) # データのみ\r\n",
    "    valid_label_tensor = torch.tensor(_valid_label)\r\n",
    "    # print(valid_data_tensor.size(),valid_label_tensor.size())\r\n",
    "    # print(\"---------\")\r\n",
    "    valid_tensor = data.TensorDataset(valid_data_tensor, valid_label_tensor)\r\n",
    "    validloader = data.DataLoader(valid_tensor, batch_size = 64)\r\n",
    "    # ここまででデータの準備完了------------------------------------------------\r\n",
    "    # ここから学習\r\n",
    "    \r\n",
    "    for epoch in range(n_epochs):\r\n",
    "        train_loss = 0\r\n",
    "        train_acc = 0\r\n",
    "        valid_loss = 0\r\n",
    "        valid_acc = 0\r\n",
    "        auc_score = 0\r\n",
    "        \r\n",
    "        model.train() # 学習モード <-これテキストとかにあった気がしないがなんだ？？\r\n",
    "        # dropoutやbatch normのふるまいが変わる。今回はNNに組み込んでいないから必要ないと思われる\r\n",
    "        batch_count = 0\r\n",
    "        for xt, yt in trainloader: # ミニバッチずつ計算\r\n",
    "            # データをgpuに <-これであってる？？\r\n",
    "            xt = xt.to(device)\r\n",
    "            yt = yt.to(device)\r\n",
    "            \r\n",
    "            y_pred = model.forward(xt) # モデルにミニバッチをぶち込む\r\n",
    "            loss = loss_fn(y_pred, yt) # 予測結果の損失計算\r\n",
    "            # print(f\"{batch_count + 1}回目のミニバッチごとのロス:{loss}\")\r\n",
    "            train_loss += loss.item() * xt.size(0) # ミニバッチのサイズを掛ける\r\n",
    "            train_acc += (y_pred.max(1)[1] == yt).sum().item()\r\n",
    "\r\n",
    "            optimizer.zero_grad() # 勾配情報の初期化\r\n",
    "            loss.backward() # 誤差逆伝搬\r\n",
    "            optimizer.step() # 重みの更新\r\n",
    "            # print(batch_count)\r\n",
    "            batch_count += 1\r\n",
    "        # print(batch_count)\r\n",
    "        \r\n",
    "        avg_train_loss = train_loss / len(trainloader.dataset)\r\n",
    "        avg_train_acc = train_acc / len(trainloader.dataset)\r\n",
    "        \r\n",
    "        # validation\r\n",
    "        model.eval() # <- dropoutやbatch normのふるまいが変わる。今回はNNに組み込んでいないから必要ないと思われる\r\n",
    "        with torch.no_grad(): # validationにおいては勾配を計算する必要がない\r\n",
    "            for xv, yv in validloader:\r\n",
    "                xv = xv.to(device)\r\n",
    "                yv = yv.to(device)\r\n",
    "                y_pred = model(xv)\r\n",
    "                loss = loss_fn(y_pred, yv)\r\n",
    "                valid_loss += loss.item() * xv.size(0) # ミニバッチのサイズを掛ける\r\n",
    "                valid_acc += (y_pred.max(1)[1] == yv).sum().item()\r\n",
    "                \r\n",
    "\r\n",
    "            avg_valid_loss = valid_loss / len(validloader.dataset)\r\n",
    "            avg_valid_acc = valid_acc / len(validloader.dataset)\r\n",
    "            avg_auc_score = auc_score / len(validloader.dataset)\r\n",
    "        if epoch == 0 or (epoch + 1) % 50 == 0:\r\n",
    "            print(f\"epoch:{epoch + 1},train_loss:{avg_train_loss:.5f}, train_acc:{avg_train_acc:.5f}, val_loss:{avg_valid_loss:.5f},val_acc:{avg_valid_acc:.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is 1th fold.--------------------\n",
      "epoch:1,train_loss:0.70691, train_acc:0.39198, val_loss:0.70297,val_acc:0.39506\n",
      "epoch:50,train_loss:0.66939, train_acc:0.60802, val_loss:0.67088,val_acc:0.60494\n",
      "epoch:100,train_loss:0.66952, train_acc:0.60802, val_loss:0.67088,val_acc:0.60494\n",
      "This is 2th fold.--------------------\n",
      "epoch:1,train_loss:0.70659, train_acc:0.39198, val_loss:0.70281,val_acc:0.39506\n",
      "epoch:50,train_loss:0.66982, train_acc:0.60802, val_loss:0.67093,val_acc:0.60494\n",
      "epoch:100,train_loss:0.66948, train_acc:0.60802, val_loss:0.67094,val_acc:0.60494\n",
      "This is 3th fold.--------------------\n",
      "epoch:1,train_loss:0.70238, train_acc:0.39198, val_loss:0.69912,val_acc:0.39506\n",
      "epoch:50,train_loss:0.66811, train_acc:0.60802, val_loss:0.67087,val_acc:0.60494\n",
      "epoch:100,train_loss:0.67027, train_acc:0.60802, val_loss:0.67088,val_acc:0.60494\n",
      "This is 4th fold.--------------------\n",
      "epoch:1,train_loss:0.69562, train_acc:0.40278, val_loss:0.69258,val_acc:0.61111\n",
      "epoch:50,train_loss:0.66985, train_acc:0.60648, val_loss:0.66746,val_acc:0.61111\n",
      "epoch:100,train_loss:0.64765, train_acc:0.60648, val_loss:0.64570,val_acc:0.61111\n",
      "This is 5th fold.--------------------\n",
      "epoch:1,train_loss:0.69631, train_acc:0.39198, val_loss:0.69378,val_acc:0.38889\n",
      "epoch:50,train_loss:0.67022, train_acc:0.60648, val_loss:0.66837,val_acc:0.61111\n",
      "epoch:100,train_loss:0.67032, train_acc:0.60648, val_loss:0.66837,val_acc:0.61111\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "num = 0\r\n",
    "n_epochs = 5\r\n",
    "for train_idx, valid_idx in skf.split(train_data, train_label):\r\n",
    "    print(f\"This is {num + 1}th fold.--------------------\")\r\n",
    "    num += 1\r\n",
    "    # foldごとのモデルの初期化\r\n",
    "    model = Net_bn().to(device)\r\n",
    "    # foldごとにモデルの最適化手法のパラメータ引数をちゃんと設定してあげる必要があった\r\n",
    "    optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9, weight_decay = 5e-3)\r\n",
    "\r\n",
    "    # データの準備-------------------------------------------------------------\r\n",
    "    # train_idx, valid_idxにはインデックスの値が格納されている。\r\n",
    "    _train_data = train_data.iloc[train_idx]\r\n",
    "    _train_label = train_label.iloc[train_idx]\r\n",
    "    _valid_data = train_data.iloc[valid_idx]\r\n",
    "    _valid_label = train_label.iloc[valid_idx]\r\n",
    "    # データの準備\r\n",
    "    # 訓練データをtensor型に変換------------\r\n",
    "    train_data_tensor = torch.tensor(np.array(_train_data.astype('f'))) # データのみ\r\n",
    "    train_label_tensor = torch.tensor(_train_label)\r\n",
    "    # DataLoaderに渡す\r\n",
    "    train_tensor = data.TensorDataset(train_data_tensor, train_label_tensor)\r\n",
    "    trainloader = data.DataLoader(train_tensor, batch_size = 64)\r\n",
    "    # valid\r\n",
    "    valid_data_tensor = torch.tensor(np.array(_valid_data.astype('f'))) # データのみ\r\n",
    "    valid_label_tensor = torch.tensor(_valid_label)\r\n",
    "    # print(valid_data_tensor.size(),valid_label_tensor.size())\r\n",
    "    # print(\"---------\")\r\n",
    "    valid_tensor = data.TensorDataset(valid_data_tensor, valid_label_tensor)\r\n",
    "    validloader = data.DataLoader(valid_tensor, batch_size = 64)\r\n",
    "    # ここまででデータの準備完了------------------------------------------------\r\n",
    "    # ここから学習\r\n",
    "    \r\n",
    "    for epoch in range(n_epochs):\r\n",
    "        train_loss = 0\r\n",
    "        train_acc = 0\r\n",
    "        valid_loss = 0\r\n",
    "        valid_acc = 0\r\n",
    "        auc_score = 0\r\n",
    "        \r\n",
    "        model.train() # 学習モード <-これテキストとかにあった気がしないがなんだ？？\r\n",
    "        # dropoutやbatch normのふるまいが変わる。今回はNNに組み込んでいないから必要ないと思われる\r\n",
    "        batch_count = 0\r\n",
    "        for xt, yt in trainloader: # ミニバッチずつ計算\r\n",
    "            # データをgpuに <-これであってる？？\r\n",
    "            xt = xt.to(device)\r\n",
    "            yt = yt.to(device)\r\n",
    "            \r\n",
    "            y_pred = model.forward(xt) # モデルにミニバッチをぶち込む\r\n",
    "            loss = loss_fn(y_pred, yt) # 予測結果の損失計算\r\n",
    "            # print(f\"{batch_count + 1}回目のミニバッチごとのロス:{loss}\")\r\n",
    "            train_loss += loss.item() * xt.size(0) # ミニバッチのサイズを掛ける\r\n",
    "            _, y_pred = torch.max(y_pred, 1)\r\n",
    "            train_acc += torch.sum(y_pred == yt)\r\n",
    "            optimizer.zero_grad() # 勾配情報の初期化\r\n",
    "            loss.backward() # 誤差逆伝播\r\n",
    "            optimizer.step() # 重みの更新\r\n",
    "            # print(batch_count)\r\n",
    "            batch_count += 1\r\n",
    "        # print(batch_count)\r\n",
    "        \r\n",
    "        avg_train_loss = train_loss / len(trainloader.dataset)\r\n",
    "        avg_train_acc = train_acc / len(trainloader.dataset)\r\n",
    "        \r\n",
    "        # validation\r\n",
    "        model.eval() # <- dropoutやbatch normのふるまいが変わる。今回はNNに組み込んでいないから必要ないと思われる\r\n",
    "        with torch.no_grad(): # validationにおいては勾配を計算する必要がない\r\n",
    "            for xv, yv in validloader:\r\n",
    "                xv = xv.to(device)\r\n",
    "                yv = yv.to(device)\r\n",
    "                y_pred = model(xv)\r\n",
    "                loss = loss_fn(y_pred, yv)\r\n",
    "                valid_loss += loss.item() * xv.size(0) # ミニバッチのサイズを掛ける\r\n",
    "                _, y_pred = torch.max(y_pred, 1)\r\n",
    "                valid_acc += torch.sum(y_pred == yv)\r\n",
    "                \r\n",
    "\r\n",
    "            avg_valid_loss = valid_loss / len(validloader.dataset)\r\n",
    "            avg_valid_acc = valid_acc / len(validloader.dataset)\r\n",
    "            avg_auc_score = auc_score / len(validloader.dataset)\r\n",
    "        print(f\"epoch:{epoch + 1},train_loss:{avg_train_loss:.5f}, train_acc:{avg_train_acc:.5f}, val_loss:{avg_valid_loss:.5f},val_acc:{avg_valid_acc:.5f}\")\r\n",
    "        # if epoch == 0 or (epoch + 1) % 10 == 0:\r\n",
    "        #     print(f\"epoch:{epoch + 1},train_loss:{avg_train_loss:.5f}, train_acc:{avg_train_acc:.5f}, val_loss:{avg_valid_loss:.5f},val_acc:{avg_valid_acc:.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is 1th fold.--------------------\n",
      "epoch:1,train_loss:0.66745, train_acc:0.59105, val_loss:0.68919,val_acc:0.60494\n",
      "epoch:2,train_loss:0.47526, train_acc:0.91667, val_loss:0.70049,val_acc:0.41975\n",
      "epoch:3,train_loss:0.38710, train_acc:0.98765, val_loss:0.66435,val_acc:0.63580\n",
      "epoch:4,train_loss:0.34860, train_acc:0.99846, val_loss:0.67228,val_acc:0.60494\n",
      "epoch:5,train_loss:0.33344, train_acc:1.00000, val_loss:0.64697,val_acc:0.63580\n",
      "This is 2th fold.--------------------\n",
      "epoch:1,train_loss:0.66652, train_acc:0.60340, val_loss:0.69238,val_acc:0.52469\n",
      "epoch:2,train_loss:0.49320, train_acc:0.92901, val_loss:0.67285,val_acc:0.69136\n",
      "epoch:3,train_loss:0.39594, train_acc:0.98611, val_loss:0.62312,val_acc:0.70370\n",
      "epoch:4,train_loss:0.34810, train_acc:1.00000, val_loss:0.60226,val_acc:0.68519\n",
      "epoch:5,train_loss:0.33129, train_acc:1.00000, val_loss:0.59804,val_acc:0.69753\n",
      "This is 3th fold.--------------------\n",
      "epoch:1,train_loss:0.66356, train_acc:0.58179, val_loss:0.68495,val_acc:0.60494\n",
      "epoch:2,train_loss:0.47045, train_acc:0.92438, val_loss:0.66206,val_acc:0.61111\n",
      "epoch:3,train_loss:0.38191, train_acc:0.98920, val_loss:0.62953,val_acc:0.65432\n",
      "epoch:4,train_loss:0.34096, train_acc:1.00000, val_loss:0.59949,val_acc:0.72840\n",
      "epoch:5,train_loss:0.32779, train_acc:1.00000, val_loss:0.58730,val_acc:0.72222\n",
      "This is 4th fold.--------------------\n",
      "epoch:1,train_loss:0.69575, train_acc:0.52932, val_loss:0.68988,val_acc:0.69753\n",
      "epoch:2,train_loss:0.48450, train_acc:0.90895, val_loss:0.65877,val_acc:0.64815\n",
      "epoch:3,train_loss:0.39477, train_acc:0.98920, val_loss:0.60246,val_acc:0.74074\n",
      "epoch:4,train_loss:0.34994, train_acc:0.99691, val_loss:0.57679,val_acc:0.74074\n",
      "epoch:5,train_loss:0.33058, train_acc:1.00000, val_loss:0.55808,val_acc:0.72840\n",
      "This is 5th fold.--------------------\n",
      "epoch:1,train_loss:0.67752, train_acc:0.58179, val_loss:0.69691,val_acc:0.38889\n",
      "epoch:2,train_loss:0.52764, train_acc:0.85802, val_loss:0.65791,val_acc:0.66667\n",
      "epoch:3,train_loss:0.40775, train_acc:0.97222, val_loss:0.62315,val_acc:0.66049\n",
      "epoch:4,train_loss:0.35587, train_acc:0.98765, val_loss:0.58941,val_acc:0.68519\n",
      "epoch:5,train_loss:0.33461, train_acc:1.00000, val_loss:0.56771,val_acc:0.72222\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 過学習に対処するには？\r\n",
    "\r\n",
    "train_accの挙動は気にしなくてよい。valid_accが下がらなければ過学習ではない。\r\n",
    "[double descent][https://www.acceluniverse.com/blog/developers/2020/01/deep-double-descent-where-bigger-models-and-more-data-hurt.html]"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c939a731ced144f499ba8faa07b12f85fe28d8c853ba633eb8f0980b6dd55f84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}